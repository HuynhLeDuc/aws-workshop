[
{
	"uri": "http://localhost:1313/aws-workshop/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Modernizing Java Applications with Amazon Q Developer and Visual Studio Code by Vimal Vyas and Shweta Singh | on 01 APR 2025 | in\rAmazon Q, Amazon Q Developer, AWS Java Development, DevOps, Technical How-to | Permalink\rAs time progresses, organizations continue using Java applications that were built many years ago using older versions of Java Development Kits (JDKs). This result in applications running deprecated code with outdated dependencies. This combination of factors might lead to security vulnerabilities, poor application performance and maintenance challenges. These challenges are increasingly difficult to manage in organizations running a large number of Java applications. Developers are looking for a development environment that facilitates running old Java applications with different JDKs; and tools that help migrate the application to the current Java version automatically, replacing the deprecated code and fix missing dependencies.\nIn April, 2024, Amazon Web Services (AWS) announced the general availability of Amazon Q Developer: Transform for Java. On February 14, 2025, Amazon Q Developer announced support for upgrades to Java 21. With this release, Amazon Q Developer: Transform adds a single-click transformation of old Java 8 and Java 11 applications to Java 21.\nThis post shows how to set up Visual Studio Code (VSCode) with multiple versions of JDK. Also, VSCode’s integration with Amazon Q Developer: Transform to automate the transformation of Java applications to current versions.\nOverview of the solution The solution consists of the following steps to set up your development environment:\nConfigure Visual Studio Code with multiple JDKs and Maven Integrate Visual Studio Code with Amazon Q Developer: Transform After you set up the development environment, use Amazon Q Developer: Transform feature to transform Java 8 or Java 11 applications to Java 21.\nSetting Up Your Development Environment Configure Visual Studio Code (VSCode) with Multiple JDKs In VSCode, select the View/Command Palette then chose Java:Configure Java runtime and download JDK versions 8, 11 and 21. As an option, use Amazon Corretto to download multiple versions of JDK. Amazon Corretto is a no-cost, multiplatform, production-ready distribution of OpenJDK.\nTo install the Maven plugin, use VSCode Extensions, type or select Maven for Java and select Install. Now add your Java applications built on Java 8 and Java 11 to a VSCode workspace. We are using sample java demo applications developed with Java 8 and Java 11 versions for this walkthrough. Make sure that the pom.xml file points to the applicable version of Java as shown in the following image. Integrating Visual Studio Code with Amazon Q Developer: Transform Amazon Q Developer provides integration with multiple integrated development environments (IDEs) and VSCode is one of them. Follow the AWS documentation to integrate VSCode with Amazon Q Developer.\nTransforming Java Applications Java 8 to Java 21 Transformation The following steps will guide you to perform the code transformation:\nChoose Amazon Q in the navigation bar from the bottom panel of your VSCode.\nThen select Open Chat panel from the command palette. In the chat panel, type /transform and then choose the Java 8 project to transform.\nNow select source code version as 8 and target code version as 21. Choose Confirm.\nA pop-up window will open titled choose to skip unit tests, select Skip unit tests and choose Confirm. We will skip unit test because there is a technical limitation where the project must be able to build and complete its tests within 55 minutes.\nA new pop-up window opens and provides options to choose how to receive proposed changes. It provides the options One Diff and Multiple Diffs. Select One Diff for smaller-scale transformations with manageable code changes.\nA final pop-up window opens to provide the command to find the JDK path. Run the command in a new terminal and then enter the path to JDK 8. Once you enter the JDK path, Amazon Q starts building your project. This can take up to 10 minutes depending on the size of your project. Review Transformation plan Once Amazon Q Developer uploads and builds your code in a secure build environment, it generates a transformation plan. The transformation plan displays Java code details including lines of code, dependencies to replace, deprecated Java 8 code instances and number of files to be changed.\nReview Transformation Summary Upon completion of the transformation, Amazon Q Developer generates a detailed transformation summary listing the dependencies, Java 8 deprecated code instances and list of all the files changed. The following instances of deprecated code were replaced by Amazon Q Developer.\nAmazon Q Developer summarizes the proposed changes in the Proposed Changes tab, that opens up after the transformation is complete. To accept the changes Amazon Q made, go to the Proposed Changes tab and choose Accept. Java 11 to Java 21 Transformation To transform Java 11, follow the same steps described above. This time, select Java 11 as the source version. The process includes:\n· Project selection and version specification\n· Build environment setup\n· Transformation plan review\n· Accept the final changes\nUnit Testing Enterprises also face challenges with high defect rates, quality issues, and long development cycles for new features and applications. Amazon Q Developer helps development teams achieve improvements in productivity by automating mundane tasks and reducing technical debt accumulation through the /test feature.\nThe /test feature in Amazon Q Developer is an automated unit test generation capability that helps developers create comprehensive test suites. The following is an example of creating a unit test:\nTo run the tests, navigate to the Testing tab, and select the generated java test class. From this testing tab, you have complete control to run any of the tests, either a single test case or multiple test cases.\nBenefits and Impact Amazon Q Developer serves as a powerful AI-powered assistant that helps transform and modernize software development across the entire lifecycle delivering the following benefits –\n· Accelerate code modernization\n· Simplify dependency management\n· Reduce technical debt\n· Enhance application performance\n· Streamline maintenance\nConclusion The Amazon Q Developer Agent for Code Transformation (QCT) for Java helps modernize legacy Java applications faster and more efficiently. This tool transforms outdated systems to current frameworks and deploys them as cloud-native applications on AWS. This process minimizes effort, risk, and ongoing maintenance needs. By using Amazon Q Developer, you’ll free up time and resources previously spent managing technical debt, allowing your team to concentrate on innovation and enhancing your newly modernized applications.\nAs demonstrated in this post, Developers can transform multiple java applications built on different JDK versions using Amazon Q Developer and VSCode. Developers can also leverage /test feature of Amazon Q Developer to quickly generate unit test cases. This tool reduces the complexity and time required to modernize legacy Java applications enabling developers to focus on innovation rather than maintenance.\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Modernizing trading workloads with next-generation AWS Outposts racks by Jim Cotis | on 01 MAY 2025 | in\r"
},
{
	"uri": "http://localhost:1313/aws-workshop/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Modify Amazon EBS volumes on Kubernetes with Volume Attributes Classes by pmm on 05 MAR 2025 in | Containers, Technical How-to, Permalink, This post was jointly authored by Kevin Liu (Senior PMT), Jens-Uwe Walther (Senior STAM-Containers), and Drew Sirenko (Software Dev Engineer).\nIntroduction In this post, we explore how to modify Amazon Elastic Block Store (Amazon EBS) volumes on Kubernetes without application downtime. Learn how to use the VolumeAttributesClass API alongside the Amazon EBS Container Storage Interface (CSI) driver to tune provisioned performance, migrate to gp3 volumes, and automate your data backup workflows.\nModern containerized applications that use persistent storage, such as data analytics, databases, or video encoding and decoding, need a wide variety of storage characteristics such as low latency, high throughput, or more. Amazon EBS is a good fit for these workloads. EBS volumes offer different storage types, configurable storage attributes like IOPS and throughput, and the ability to modify these properties without downtime.\nIf you deploy your stateful container workloads on Kubernetes, then you can use the Amazon EBS CSI driver to provision and manage EBS volumes on your behalf. When you create Persistent Volume Claim (PVC) resources with a specified size and a Storage Class (SC), Kubernetes works with the Amazon EBS CSI driver to deploy your workloads with the necessary storage by creating, attaching, and formatting EBS volumes on your behalf.\nChanging storage characteristics of volumes in Kubernetes used to be an offline action, necessitating that cluster operators create another SC, and migrate to new PVC and Persistent Volume (PV) resources. This process necessitated application downtime and impacted availability. That’s why, in 2023, AWS released the volume-modifier-for-k8s sidecar, enabling online updates of volumes’ types and performance characteristics by applying annotations to PVCs. Although it was important to provide a solution to users, AWS worked with other storage providers to create a volume modification solution that was available as a native Kubernetes API. Over the past year AWS has worked with the Kubernetes community to release the VolumeAttributesClass Kubernetes Enhancement, which has reached beta in Kubernetes 1.31. Although the feature is disabled by default in upstream Kubernetes, it is automatically enabled for you in your Amazon Elastic Kubernetes Service (Amazon EKS) 1.31 clusters.\nStarting in Amazon EKS 1.31, you can use the VolumeAttributesClass (VAC) feature to modify the volume type, IOPS, or throughput of your EBS volumes without the volume-modifier-for-k8s sidecar. Moreover, you can use VACs to add, edit, and remove AWS resource tags from your volumes. The Amazon EBS CSI driver enables this feature by default when you upgrade to version 1.35.0 or later.\nUsing Amazon EKS 1.31 and Amazon EBS CSI driver 1.35.0, you can already start using VACs instead of PVC annotations. In the next two sections, we introduce VolumeAttributesClasses and show you how to enable the feature on your cluster. Then, we walk through three common workflows for modifying your volumes with VACs:\nTuning the throughput and input output operations per second (IOPS) performance characteristics of your volumes. Migrating to EBS gp3 volumes for up to 20% lower price per GB as compared to gp2 volumes. Modifying your EBS volume resource tags to automate your data backup workflow with Amazon Data Lifecycle Manager. Solution overview Cluster operators can rely on the Amazon EBS CSI driver to declaratively manage their persistent volumes. You can request persistent storage for your workloads by creating PVC resources with a particular size, SC, and VAC.\nA VAC is composed of its name, the driverName of the CSI driver, and a list of mutable parameters specific to a storage-provider such as Amazon EBS.\nMutable EBS volume parameters, such as throughput, IOPS, volume types, and AWS resource tags can be specified in VAC resources:\napiVersion: storage.k8s.io/v1beta1\nkind: VolumeAttributesClass\nmetadata:\nname: ebs-blog-gp3-standard-performance\ndriverName: ebs.csi.aws.com\nparameters:\ntype: gp3\niops: \u0026ldquo;3000\u0026rdquo;\nthroughput: \u0026ldquo;125\u0026rdquo;\ntagSpecification_1: \u0026ldquo;performance=standard\u0026rdquo;\nYou can add a VAC to a PVC using the spec.volumeAttributesClassName field:\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: ebs-blog-overview\nspec: \u0026hellip;\nvolumeAttributesClassName: ebs-blog-gp3-standard-performance\nThe volume is provisioned with the parameters specified in both the VAC and SC, with the VAC parameters taking precedence over any conflicting parameters. A PVC’s VAC also overrides all annotation-based modifications, and prevents future annotation-based modifications.\nThis separate VAC resource benefits organizations that split responsibilities between teams that operate the cluster and teams that deploy applications to it. Cluster operators can create cluster-wide SC and VAC objects, which then can be consumed by application developers in their name-spaced PVC resources.\nPrerequisites On Amazon EKS 1.31 or later, the beta VolumeAttributesClass feature gate and storage.k8s.io/v1beta1 API group are automatically enabled on your cluster’s control plane components. Furthermore, you must use Amazon EBS CSI driver version v1.35.0 or later, which is installed by Amazon EKS Managed add-on v1.35.0-eksbuild.2 or Helm chart v2.35.1.\nIf you operate a self-managed (for example, kOps) Kubernetes v1.31 cluster, you must enable the VolumeAttributesClass feature gate on the kube-apiserver, kube-scheduler, and kube-controller-manager, as well as enable the storage.k8s.io/v1beta1 API group through the kube-apiserver runtime-config.\nConsult the Kubernetes VolumeAttributesClass documentation for the complete list of requirements.\nTo modify Amazon EBS resource tags through VACs, make sure that you attach the following AWS Identity and Access Management (IAM) Policy to the role used by your Amazon EBS CSI driver:\n{\n\u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;,\n\u0026ldquo;Statement\u0026rdquo;: [\n{\n\u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;,\n\u0026ldquo;Action\u0026rdquo;: [\n\u0026ldquo;ec2:CreateTags\u0026rdquo;\n],\n\u0026ldquo;Resource\u0026rdquo;: [\n\u0026ldquo;arn:aws:ec2:::volume/\u0026rdquo;,\n\u0026ldquo;arn:aws:ec2:::snapshot/\u0026rdquo;\n]\n}\n]\n}\nWalkthrough The following steps walk you through this solution.\nProvision a volume, and increase its performance Amazon EKS 1.31 doesn’t come with an SC by default. We must create the following SC:\n$ kubectl apply -f - \u0026laquo;EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: ebs-blog-sc\nprovisioner: ebs.csi.aws.com\nallowVolumeExpansion: true\nEOF\nHere are two example VAC resources, each with different volume quality-of-service parameters:\n$ kubectl apply -f - \u0026laquo;EOF\napiVersion: storage.k8s.io/v1beta1\nkind: VolumeAttributesClass\nmetadata:\nname: ebs-blog-gp3-standard-performance\ndriverName: ebs.csi.aws.com\nparameters:\ntype: gp3\niops: \u0026ldquo;3000\u0026rdquo;\nthroughput: \u0026ldquo;125\u0026rdquo;\napiVersion: storage.k8s.io/v1beta1\nkind: VolumeAttributesClass\nmetadata:\nname: ebs-blog-gp3-increased-performance\ndriverName: ebs.csi.aws.com\nparameters:\ntype: gp3\niops: \u0026ldquo;4000\u0026rdquo;\nthroughput: \u0026ldquo;130\u0026rdquo;\nEOF\nProvision a volume with the standard performance VAC:\n$ kubectl apply -f - \u0026laquo;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: ebs-blog-claim\nspec: accessModes:\n- ReadWriteOnce\nstorageClassName: ebs-blog-sc\nvolumeAttributesClassName: ebs-blog-gp3-standard-performance\nresources:\nrequests: storage: 10Gi\nEOF\nFor higher performance, you can patch the PVC to point to the new ebs-blog-gp3-increased-performance VAC:\n$ kubectl patch pvc ebs-blog-claim -p \u0026lsquo;{\u0026ldquo;spec\u0026rdquo;: { \u0026ldquo;volumeAttributesClassName\u0026rdquo;: \u0026ldquo;ebs-blog-gp3-increased-performance\u0026rdquo; }}\u0026rsquo;\nThe Amazon EBS CSI driver observes that the PVC’s current and desired VACs differ, modifies the EBS volume, and updates the PV resource to the new VAC.\nYou may only perform one volume modification per six hour period according to Amazon EBS documentation. Therefore, you should merge PVC storage size increases with VAC changes into one Kubernetes patch:\n$ kubectl patch pvc ebs-blog-claim \u0026ndash;type json -p \u0026lsquo;[{\u0026ldquo;op\u0026rdquo;: \u0026ldquo;replace\u0026rdquo;, \u0026ldquo;path\u0026rdquo;: \u0026ldquo;/spec/volumeAttributesClassName\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;ebs-blog-gp3-increased-performance\u0026rdquo; }, {\u0026ldquo;op\u0026rdquo;: \u0026ldquo;replace\u0026rdquo;, \u0026ldquo;path\u0026rdquo;: \u0026ldquo;/spec/resources/requests/storage\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;11Gi\u0026rdquo;}]\u0026rsquo;\nOtherwise, you observe the following failure event on the PVC resource:\nVolumeResizeFailed: \u0026ldquo;pvc-716766d1-58d5-411b-8d4a-7671ef9894e9\u0026rdquo; by resizer \u0026ldquo;ebs.csi.aws.com\u0026rdquo; failed… VolumeModificationRateExceeded: You\u0026rsquo;ve reached the maximum modification rate per volume limit. Wait at least 6 hours between modifications per EBS volume.\nSave costs by migrating to gp3 volumes VolumeAttributesClasses can be used with existing PersistentVolumeClaims created without a VAC. This is especially useful for migrating from gp2 to gp3 volumes. EBS gp3 volumes allow you to provision IOPS and throughput independent of storage size. You can migrate to gp3 volumes in any use case where gp2 volumes were used, and may see cost savings of up to 20% because of this independence. If you are looking for even higher performance, then you can scale gp3 volumes up to 1,000 MiB/s, four times higher than the maximum throughput of gp2 volumes.\nHere we create a stateful workload that initially relies on a gp2 volume:\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ebs-blog-migration spec: accessModes: - ReadWriteOnce storageClassName: ebs-blog-migration resources: requests: storage: 1Gi --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ebs-blog-migration provisioner: ebs.csi.aws.com allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer parameters: type: gp2 --- apiVersion: v1 kind: Pod metadata: name: ebs-blog-migration-app spec: containers: - name: ebs-blog-app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;] volumeMounts: - name: persistent-storage mountPath: /data volumes: - name: persistent-storage persistentVolumeClaim: claimName: ebs-blog-migration EOF To migrate this volume to gp3, first apply the VAC resource to your cluster:\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: storage.k8s.io/v1beta1 kind: VolumeAttributesClass metadata: name: ebs-blog-gp3 driverName: ebs.csi.aws.com parameters: type: gp3 EOF Then patch its associated PVC:\n$ kubectl patch pvc ebs-blog-migration -p \u0026lsquo;{\u0026ldquo;spec\u0026rdquo;: {\u0026ldquo;volumeAttributesClassName\u0026rdquo;: \u0026ldquo;ebs-blog-gp3\u0026rdquo;}}\u0026rsquo; Note, as of Kubernetes version 1.31, you cannot modify volumes that reference an in-tree SC via VAC. Ensure that the SC associated with the volume you are modifying references provisioner ebs.csi.aws.com, not kubernetes.io/aws-ebs.\nWorkflow modify tags To ease management of your EBS volumes you can use tags. Tags are key-value pairs that you assign to your AWS resources, enabling you to categorize them by purpose, owner, or environment. If you have many volumes in your account, then tags can help you identify a specific volume, or group related resources. The Amazon EBS CSI driver allows you to add, replace, and clear volume resource tags through the tagSpecification VAC parameter.\nBeyond organization, volume tags can be used with Amazon Data Lifecycle Manager policies to safely back up your EBS volumes. Amazon EBS Snapshots capture the state of a volume at a specific point in time, allowing for recovery in case of data loss or corruption. To automate the creation, retention, and deletion of these snapshots, you can create a volume Amazon Data Lifecycle Manager policy targeting volumes with specific resource tags.\nFor example, the following policy creates a daily snapshot of any volume associated with theebs-blog-daily-backup VAC:\n$ aws dlm get-lifecycle-policies { \u0026#34;Policies\u0026#34;: [ { \u0026#34;PolicyId\u0026#34;: \u0026#34;policy-045203fd0b8e2bf79\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;This policy creates daily backups of my K8s PVs\u0026#34;, \u0026#34;State\u0026#34;: \u0026#34;ENABLED\u0026#34;, \u0026#34;Tags\u0026#34;: { \u0026#34;backup-interval\u0026#34;: \u0026#34;daily\u0026#34; }, \u0026#34;PolicyType\u0026#34;: \u0026#34;EBS_SNAPSHOT_MANAGEMENT\u0026#34;, \u0026#34;DefaultPolicy\u0026#34;: false } ] } Cleaning up To avoid incurring further costs, delete any example Kubernetes and AWS resources that you provisioned for these examples. Delete stateful workload example resources:\nkubectl delete pod ebs-blog-migration-app kubectl delete pvc ebs-blog-claim kubectl delete pvc ebs-blog-migration kubectl delete vac ebs-blog-gp3 kubectl delete vac ebs-blog-gp3-increased-performance kubectl delete vac ebs-blog-gp3-standard-performance kubectl delete vac ebs-blog-daily-backup kubectl delete sc ebs-blog-sc kubectl delete sc ebs-blog-migration You can confirm deletion of the EBS volumes through the Amazon Elastic Compute Cloud (Amazon EC2 ) console.\nConclusion In this post we explored how to modify Amazon EBS volumes on Kubernetes with VolumeAttributesClasses. This new feature helps you tune the performance characteristics of your stateful workloads by updating volume type, IOPS, and throughput without application downtime. Furthermore, automate your data backup workflows by modifying resource tags to match your Amazon Data Lifecycle Manager policies.\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Create Knowledge Base",
	"tags": [],
	"description": "",
	"content": "What is Knowledge Base ? Knowledge Base là gì ? Knowledge Base is \u0026ldquo;private memory\u0026rdquo; for Agent to access your specialized information, which the original AI model does not know. On the interface page of Aws Bedrock, I choose the left side Knowledge Base When you have finished configuring, wait about 5 minutes for it to initialize. This has created the Knowledge base. Note that you should configure it according to your needs and grant it the necessary permissions\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary of the “Cloud Day” Event Purpose of the Event The “Cloud Day” event was held to introduce the AI ​​development trend in Vietnam, especially the progress from Generative AI to Agentic AI. Speakers also presented new AWS solutions such as Amazon Bedrock, AgentCore, and SageMaker Unified Studio – tools to support businesses in building, deploying, and operating AI agents quickly and securely.\nSpeakers Kien Nguyen – Solutions Architect Jun Kai Loke – AI/ML Specialist SA, AWS Tamelly Lim – Storage Specialist SA, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Michael Armentano – Principal WW GTM Specialist, AWS Main Content Impact of AI on Vietnam\u0026rsquo;s economy AI is expected to contribute 120–130 billion USD (about 25% of GDP) by 2040. The current AI market size is 750 million USD, growing 15–18% per year. Vietnam has about 765 AI startups, ranked 2nd in ASEAN. Despite its great potential, Vietnam is still in the early stages and needs more investment in infrastructure, human resources, and support policies. Evolution Journey: From Generative AI → Agentic AI AI technology is moving from content creation support tools to systems capable of independent action. Agentic AI allows multiple agents to coordinate to complete complex tasks. The level of automation is increasing, gradually reducing direct human intervention. Application of Agentic AI in enterprises Increase productivity, reduce repetitive workloads, support innovation. By 2028, 33% of enterprise applications are forecast to integrate Agentic AI. About 15% of daily operational decisions can be automated. Amazon Bedrock – General AI Platform Provides many models from leading providers.\nSupports custom models with private data, ensuring security and cost optimization.\nAvailable with Responsible AI mechanism.\nProvides a fast, scalable, and easy-to-integrate AI agent deployment platform.\nAmazon Bedrock AgentCore Secure, scalable agent deployment framework.\nCompatible with popular frameworks: LangChain, CrewAI, LangGraph, Strands Agents.\nSupports short-term and long-term memory management and semantic search.\nEnables flexible integration, monitoring, and tool discovery.\nModern Data and AI Infrastructure SageMaker Unified Studio serves as the hub for data, analytics, and AI.\nTight integration with:\nRedshift, Athena, EMR, Glue – data processing and management.\nQuickSight – visualization.\nBedrock – building GenAI applications.\nSupport Zero-ETL model between S3 and Redshift, helping to simplify data flow.\nData Lakehouse model Support diverse storage systems: S3 Tables, Redshift Managed Storage. Connect data from many large sources such as Aurora, DynamoDB, MSK, Kinesis, Salesforce, SAP, Facebook Ads, etc. What I Learned About AI and Cloud thinking Understand the difference between Generative AI and Agentic AI – the inevitable development direction of the industry.\nRealize that Agentic AI doesn\u0026rsquo;t just answer, it also acts and makes decisions.\nUnderstand Bedrock\u0026rsquo;s capabilities in building intelligent and automated AI systems.\nAbout the technical architecture Visualize how Bedrock integrates with SageMaker, Redshift, and S3 in a complete AI ecosystem. Understand how AWS handles complex parts like memory, tool discovery, and agent observability.\nPractical Applications Use Amazon Titan Embeddings to create embeddings for an existing project.\nTest Zero-ETL models between Aurora/DynamoDB and Redshift.\nExplore the possibilities of building AI agents that automate business processes using AgentCore instead of manual Lambda.\nEvent Experience Participating in Cloud Day helped me gain a clearer perspective on how AI, especially Agentic AI, is changing the way businesses operate.\nInteract with AWS experts Speakers shared many practical examples of how Amazon applies AI at scale.\nDeeper understanding of how multi-agent systems operate in enterprise environments.\nTechnical Experience Learn how AgentCore works from memory management to tool integration.\nUnderstand how data is transferred between S3 – Redshift – SageMaker in AI applications.\nUnderstand the Lakehouse and Zero-ETL models.\nLessons Learned Agentic AI is an inevitable step forward if businesses want to automate at scale.\nModern AI infrastructure must be based on cloud and data.\nAWS is leading the way in providing a complete platform for AI/ML.\nEvent Photos Overall, the event not only provided me with technical knowledge but also helped me change my approach to application design, modernize systems and improve my ability to coordinate between teams. This was a truly valuable experience, helping me gain a clearer understanding of how to build effective and flexible solutions in an enterprise environment.\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: “Generative AI with Amazon Bedrock” Workshop Objectives Provide basic knowledge about Generative AI and its differences from classical Machine Learning. Describe in detail the Amazon Bedrock service and Foundation Models. Guide to RAG (Retrieval Augmented Generation) techniques to develop intelligent, accurate AI applications and minimize illusion errors. Introduce the system of specialized AI services of AWS. List of presenters Lam Tuan Kiet - Senior DevOps Engineer, FPT Software. Danh Hoang Hieu Nghia - AI Engineer, Renova Cloud. Dinh Le Hoang Anh - Cloud Engineering Intern, First Cloud AI Journey. Main content Transformation trends: Traditional ML models and Background models Traditional ML models: Focus on specific tasks, require labeled data, complex training/deployment processes for each target.\nBackground models (FM): Trained on huge amounts of unstructured data, adaptable for many different tasks such as: Text generation, summarization, Q\u0026amp;A, Chatbot.\nAI ecosystem on AWS platform Amazon Bedrock: Platform that gathers leading FM models from AWS partners (AI21 Labs, Anthropic, Cohere, Meta, Stability AI,\u0026hellip;) and Amazon\u0026rsquo;s own models.\nAWS Specialized AI Services: Can be considered as \u0026ldquo;off-the-shelf solutions\u0026rdquo;, optimized for each specific task without the need to train a model:\nAmazon Rekognition: Object recognition, Face recognition, Emotion recognition, Celebrity recognition, Video analysis - $0.001/image for the first 1 million images.\nAmazon Translate: Real-time multilingual text translation with high accuracy and natural writing style.\nAmazon Textract: Extract structured information (tables, forms) from scanned documents or PDFs.\nAmazon Transcribe: Convert speech to text.\nAmazon Polly: Convert text to speech.\nAmazon Comprehend: Text sentiment analysis, keyword extraction and automatic topic classification.\nAmazon Kendra: Enables natural language search of internal documents.\nAmazon Lookout: Detect anomalies in production lines or industrial equipment for predictive maintenance.\nAmazon Personalize: Build real-time recommendation systems based on machine learning.\nPrompting Technique: Chain of Thought (CoT) Comparison between Standard Prompting (directly asking for the result) and Chain of Thought Prompting.\nCoT guides the reasoning model step by step to solve complex logic problems, significantly improving accuracy compared to just providing the final answer.\nRAG (Retrieval Augmented Generation) Technique – Technical Focus Problem: Solve the \u0026ldquo;illusion\u0026rdquo; and lack of updated information of LLM models.\nSolution: Combine the retrieval ability (Retrieval) from an external Knowledge Base with the generation ability (Generation) of LLM.\nData Ingestion Process:\nRaw data (New data) → Chunking. Process via Embedding Model (e.g. Amazon Titan Text Embeddings V2.0). Store as vectors in Vector Repositories (OpenSearch Serverless, Pinecone, Redis\u0026hellip;). RetrieveAndGenerate API: API manages the entire process from receiving user input → creating query embedding → retrieving data → adding context (augment prompt) → generating answers. Acquired knowledge About AI Thinking and Cloud Understand when to use Specialized AI Services for quick, specific problems and when to use Bedrock/GenAI for creative, complex problems.\nMaster RAG system design thinking: It\u0026rsquo;s not just about calling LLM APIs, but about data management and vectorization to provide the correct context for AI to generate better responses.\nAbout Technical Architecture CoT Technique is the key to optimizing model output without fine-tuning.\nDeep understanding of the role of Amazon Titan Embeddings V2.0 in converting multilingual text to vectors (supports 100+ languages, flexible vector size 256/512/1024).\nWork application plan Amazon Bedrock applications for current projects: Amazon Rekognition to recognize dishes from photos to automatically fill in calorie information; Amazon Comprehend to analyze text to standardize dish names and record calorie data. Experiment with applying RAG techniques for current projects. Use Bedrock Agents to coordinate tasks such as querying dishes from vector warehouses, calculating calorie targets, and building daily menus. Event participation experience Participating in the Generative AI workshop with Amazon Bedrock provides a practical perspective on how to build modern AI applications, from fundamental theory to practical implementation.\nPractical knowledge from experts The speakers clearly presented the data flow in a RAG ​​system, helping me visualize the workings behind current Chatbot applications.\nA clear analysis of the difference between Traditional ML Model and Generative AI helps me reshape the technology selection strategy for upcoming projects.\nTechnology Experience Impressed with Bedrock\u0026rsquo;s RetrieveAndGenerate API because it significantly reduces manual programming effort for the connection between Vector Warehouse and LLM.\nSee the power of Amazon Titan Embedding in multi-language support, very suitable for applications in the Vietnamese market.\nLessons learned RAG is the new standard: To apply AI in businesses, RAG is almost mandatory to ensure data accuracy and security.\nComprehensive ecosystem: AWS provides everything from infrastructure (Vector Warehouse) to model layer (Bedrock) and application layer (Agents), making deployment much faster.\nSome Photos When Attending the Event Overall, the event not only provided technical knowledge but also helped me change the way I think about application design, modernize the system and coordinate more effectively between teams.\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/5-workshop/5.3-vpc-to-internet/5.3.1-create-gwe/",
	"title": "Exploring Bedrock",
	"tags": [],
	"description": "",
	"content": " Search Amazon Bedrock pricing I consider the price of each model and domain to see if it suits my needs Open AWS Console → Amazon Bedrock Select the model catalog, choose any model to test to suit your purpose and price I will choose Amazon Nova 2 lite here because it is cheap ($0.0003/per 1000 tokens) Select Open in Playground Try the prompt and get the result The model results are quite impressive. "
},
{
	"uri": "http://localhost:1313/aws-workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Duc Huynh\nPhone Number: 0365436310\nEmail: Huynhld.ai@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligence\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/aws-workshop/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "About Amazon Bedrock Amazon Bedrock is a fully managed machine learning service from AWS, providing access to leading Foundation Models from Anthropic (Claude), Amazon (Titan), Meta (Llama), and many others through a simple API.\nAnthropic Claude - Claude 2, Claude 3 Meta Llama - Llama 2 Amazon Titan - Titan Text AI21 Labs - Jurassic-2 Workshop Overview In this workshop, you will learn how to:\nExplore Bedrock Console\nCreate AI Agent with AWS Lambda\nExpose API to external applications\nTest with a simple web interface\nImage source\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives Connect and get acquainted with members of the First Cloud Journey (FCJ) team. Understand basic AWS services, how to create and manage costs with an AWS account. Learn how to use the AWS Console and AWS CLI to interact with and manage services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Get to know FCJ members - Read and note internship regulations and rules 08/09/2025 08/09/2025 3 - Learn about AWS and its basic service types + Compute (EC2) + Storage (S3) + Networking (VPC) + Database (RDS) 09/09/2025 09/09/2025 cloudjourney.awsstudygroup.com 4 - Create an AWS Free Tier account - Learn AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Manage identity and access permissions + Install and configure AWS CLI + Use AWS CLI with basic operations 10/09/2025 10/09/2025 cloudjourney.awsstudygroup.com 5 - Learn how to effectively manage costs using AWS Budgets + Cost Budget + Usage Budget + Reservation (RI) Budget + Savings Plans Budget - Practice: + Create Cost Budget + Create Usage Budget + Create RI Budget + Create Savings Plans Budget + Clean up unused resources 11/09/2025 11/09/2025 cloudjourney.awsstudygroup.com 6 - Learn about AWS Support service - AWS Support Plans: + Basic, Developer, Business, and Enterprise - Types of support requests: + Account and Billing support + Service Limit Increase + Technical Support - Practice: + Select Basic Support plan + Create a support case 12/09/2025 12/09/2025 cloudjourney.awsstudygroup.com Week 1 Achievements Understood what AWS is and the main service groups:\nCompute: Provides computing resources for applications such as virtual machines and containers. Storage: Used for storing, backing up, and recovering data. Networking: Manages network infrastructure, security, and connectivity between AWS resources. Database: Offers both relational and non-relational database management services. Successfully created and configured an AWS Free Tier account.\nLearned to create and manage Groups and Users in IAM.\nUnderstood how to log in using IAM and that users within the same group share the same permissions.\nBecame familiar with the AWS Management Console and how to navigate, access, and use services from the web interface.\nInstalled and configured AWS CLI on the local machine, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nChecking account and configuration information Listing available regions Creating and deleting S3 Buckets Using Amazon SNS Creating IAM Groups, Users, and adding users to groups Creating and deleting Access Keys Creating and configuring a basic VPS Running and terminating EC2 Instances Learned how to manage and monitor costs in AWS through:\nCreating and configuring Budget types (Cost, Usage, RI, Savings Plan). Cleaning up unused resources for efficient cost management. Understood the AWS Support Plans and learned how to create support requests via the AWS Support Center:\nBasic: Free, provides support for account and billing issues through the Help Center. Developer: $29/month, includes basic architectural guidance and unlimited technical support from the root account. Business: $100/month, commonly used by small and medium businesses with features such as use-case-based guidance, access to AWS Support API, and unlimited support requests from all IAM users. Enterprise: $15,000/month, for large-scale enterprises with strict security and reliability standards, offering advanced architectural, infrastructure, strategic, and cost optimization support, with priority case handling. Became comfortable using both AWS Console and AWS CLI for basic operations.\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand the concept of Amazon Virtual Private Cloud (VPC) and its importance in AWS architecture. Learn how to design, deploy, and manage a virtual private network on AWS. Learn how to set up a AWS Site-to-Site VPN connection between On-Premise and AWS Cloud environments. Master network security best practices following the AWS Well-Architected Framework. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Overview of Amazon VPC - Study AWS network architecture and the role of VPC in the cloud environment - Understand the concepts: Subnet, Route Table, Internet Gateway, NAT Gateway, VPC Peering 15/09/2025 15/09/2025 cloudjourney.awsstudygroup.com 3 - Hands-on: + Create a VPC with a standard network structure + Create Public and Private Subnets + Configure Route Table and Internet Gateway for Internet access 16/09/2025 16/09/2025 cloudjourney.awsstudygroup.com 4 - Learn about Network Security on AWS + Security Groups + Network ACLs + Compare and practice configuring access rules - Hands-on: + Create Security Groups and Network ACLs for VPC + Test access control using EC2 instances 17/09/2025 17/09/2025 cloudjourney.awsstudygroup.com 5 - Introduction to AWS Site-to-Site VPN - Study the connection model between On-premise and AWS Cloud - Hands-on: + Create Virtual Private Gateway (VGW) and Customer Gateway (CGW) + Establish VPN connection between both environments + Verify connection status and routing configuration 18/09/2025 18/09/2025 cloudjourney.awsstudygroup.com 6 - Summarize and review Week 2 learning outcomes - Advanced Practice: + Design VPC based on AWS Well-Architected Framework + Automate infrastructure deployment using Infrastructure as Code (IaC) templates (CloudFormation or Terraform) + Clean up resources after completing the workshop 19/09/2025 19/09/2025 cloudjourney.awsstudygroup.com Week 2 Achievements: Clearly understood what Amazon VPC is and the importance of network isolation in AWS Cloud.\nMastered the basic structure of a VPC, including:\nSubnets: Divide network segments for different resources. Route Tables: Define routing paths for network traffic within VPC. Internet Gateway: Enables public subnet Internet access. NAT Gateway: Allows private subnet instances to securely connect to the Internet. VPC Peering: Connects two different VPCs for resource sharing. Successfully deployed a VPC with both public and private subnets, and verified connectivity through EC2 instances.\nUnderstood and configured Security Groups and Network ACLs, differentiating their scopes and use cases.\nLearned the process of setting up a Site-to-Site VPN between on-premise systems and AWS:\nCreated and configured Virtual Private Gateway (VGW) and Customer Gateway (CGW). Established VPN connections using routing and tunnel configuration parameters. Verified VPN connection via AWS CLI and AWS Management Console. Mastered advanced network security practices following the AWS Well-Architected Framework, including:\nNetwork segmentation. Access control at both subnet and instance levels. Data encryption during transmission over VPN. Gained hands-on experience with Infrastructure as Code (IaC) to automate the creation of VPCs, Subnets, Route Tables, and Security Groups.\nCompleted the workshop by:\nDesigning and deploying a complete VPC model. Successfully establishing a Site-to-Site VPN connection. Cleaning up all AWS resources after completion. Summary of Knowledge Gained: Amazon VPC: Gained in-depth understanding of virtual networking in AWS. Network Security: Learned to configure and manage secure access with Security Groups and Network ACLs. VPN Connection: Understood how to set up and maintain secure Site-to-Site VPN connections. AWS CLI \u0026amp; Console: Practiced deploying, testing, and cleaning up resources using both tools. IaC (Infrastructure as Code): Learned how to automate AWS infrastructure deployment efficiently. "
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Understand the concept and functionality of Amazon EC2 (Elastic Compute Cloud). Learn how to launch, connect, configure, and manage EC2 instances on both Windows and Linux. Practice deploying a sample web application (AWS User Management) on EC2 instances. Learn how to monitor, secure, and clean up EC2 resources efficiently. Tasks to be carried out this week: No. Task Start Date Completion Date Reference 1 - Overview of Amazon EC2 and its key concepts: + Instance, AMI, Key Pair, Elastic IP, Security Group, Volume + EC2 pricing models (On-Demand, Spot, Reserved, Savings Plan) 22/09/2025 22/09/2025 cloudjourney.awsstudygroup.com 2 - Hands-on: Create and configure a Windows EC2 instance + Choose AMI and instance type + Configure key pair and security group + Connect using Remote Desktop (RDP) + Explore Windows Server 2022 environment 23/09/2025 23/09/2025 cloudjourney.awsstudygroup.com 3 - Hands-on: Create and configure a Linux EC2 instance + Launch Amazon Linux 2 + Connect via SSH using key pair + Explore the Linux environment and essential commands + Update system packages 24/09/2025 24/09/2025 cloudjourney.awsstudygroup.com 4 - Deploy sample app “AWS User Management” + Install Node.js, npm, and dependencies on both Linux and Windows instances + Deploy CRUD web app (User Management System) + Test Create, Read, Update, Delete, and Search functions + Share app across network using Security Groups 25/09/2025 25/09/2025 cloudjourney.awsstudygroup.com 5 - Learn EC2 monitoring and management tools: + Amazon CloudWatch (for metrics and logs) + AWS Systems Manager + EC2 Instance Connect - Clean up unused instances, Elastic IPs, and security groups 26/09/2025 26/09/2025 cloudjourney.awsstudygroup.com Week 3 Achievements: Understood the core concepts of Amazon EC2, including:\nAMI (Amazon Machine Image): Provides the OS and app template for launching instances. Instance Type: Defines computing capacity (CPU, RAM, storage). Key Pair: Used for secure login (SSH/RDP). Elastic IP: A static IP for accessing instances over the Internet. Security Group: Acts as a virtual firewall controlling inbound/outbound traffic. Successfully created and configured Windows and Linux EC2 instances:\nConnected via RDP (Windows) and SSH (Linux). Managed instances through AWS Console and CLI. Configured network rules to allow web traffic (port 80, 443, 22, 3389). Deployed the AWS User Management web application on both platforms:\nInstalled Node.js, npm, and app dependencies. Deployed a fully functional CRUD application (Add, Edit, Delete, Search users). Shared the application with other users using public IP or Elastic IP. Learned to monitor EC2 instances using:\nAmazon CloudWatch (view CPU, memory, and network usage). AWS Systems Manager for automation and instance control. EC2 Instance Connect for secure browser-based access. Practiced resource management and cost optimization:\nStopped and terminated instances when not in use. Released unused Elastic IPs. Deleted unneeded Security Groups and Key Pairs. Summary of Knowledge Gained: Amazon EC2: Deep understanding of cloud-based compute instances. Windows \u0026amp; Linux Management: Hands-on experience configuring, connecting, and securing both OS environments. Application Deployment: Deployed Node.js CRUD app on EC2 instances. Cloud Monitoring: Used CloudWatch and Systems Manager to observe performance. Cost Efficiency: Learned to clean up and optimize EC2 resources effectively. "
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives Understand how to grant application permissions to access AWS services. Learn the difference between Access Key/Secret Access Key and IAM Role. Know how to create and attach IAM Roles to EC2 instances for secure access to AWS services. Become familiar with AWS Cloud9 IDE, a cloud-based development environment. Practice writing, running, and debugging code directly in Cloud9 with AWS CLI integration. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Overview of AWS Identity and Access Management (IAM) - Understand how AWS controls access to resources - Review key concepts: User, Group, Policy, Role 2025-09-22 2025-09-22 cloudjourney.awsstudygroup.com 3 - Practice granting permissions via Access Key and Secret Access Key - Test application accessing AWS services with credentials - Analyze the security risks of embedding credentials in source code 2025-09-23 2025-09-23 cloudjourney.awsstudygroup.com 4 - Introduce and create IAM Role for EC2 Instances - Attach IAM Role and test application access to S3/DynamoDB - Practice revoking permissions and re-testing application 2025-09-24 2025-09-24 cloudjourney.awsstudygroup.com 5 - Introduction to AWS Cloud9 IDE - Create a Cloud9 environment and configure workspace - Explore features: terminal, file explorer, debugger, syntax highlighting 2025-09-25 2025-09-25 cloudjourney.awsstudygroup.com 6 - Advanced Practice: + Write AWS CLI scripts within Cloud9 + Deploy Node.js CRUD app (AWS User Management) in Cloud9 + Clean up AWS resources after completion 2025-09-26 2025-09-26 cloudjourney.awsstudygroup.com Week 4 Achievements: Fully understood the difference between Access Key/Secret Key and IAM Role, and why IAM Roles are more secure. Learned how to create and configure IAM Roles with proper permissions (e.g., read/write access to S3). Successfully attached IAM Roles to EC2 instances and accessed AWS services securely. Became familiar with AWS Cloud9 IDE: Created and configured development environments. Connected to EC2 instance. Executed AWS CLI commands and tested sample code directly in the IDE. Built and deployed a simple Node.js application on Cloud9 to interact with AWS S3 and DynamoDB. Practiced resource cleanup after workshops to avoid unnecessary billing. Summary of Knowledge Gained: IAM Role: Secure and scalable permission model for EC2. Access Key/Secret Key: Recognized risks of storing static credentials in code. AWS Cloud9: Browser-based IDE supporting multiple languages and AWS integration. AWS CLI \u0026amp; SDK: Tools for interacting programmatically with AWS services. Practical Experience: Configured, tested, and cleaned up AWS resources effectively. "
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Gain a deep understanding of Amazon S3 — object storage model, key features, and use cases. Practice hosting a static website on S3: enable the feature, configure public access, test, and optimize performance. Learn how to secure buckets (Block Public Access, IAM policy, Bucket Policy) while allowing public access for specific objects when necessary. Perform advanced management operations: Versioning, S3 Transfer Acceleration, S3 Batch / S3 Replication (cross-region replication), and object migration. Understand how to clean up resources to avoid unwanted costs and follow S3 best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 Environment setup \u0026amp; basic theory + Overview of S3: object, bucket, key, region, storage class (STANDARD, IA, GLACIER). + Durability \u0026amp; availability (11 nines). + Use cases (static website, backup, data lake). 2025-10-06 2025-10-06 cloudjourney.awsstudygroup.com 3 Create an S3 bucket \u0026amp; enable Static Website Hosting + Create a bucket (naming rules, region selection). + Upload index.html / error.html files. + Enable static website hosting and test the endpoint. 2025-10-07 2025-10-07 cloudjourney.awsstudygroup.com 4 Configure Block Public Access \u0026amp; Bucket Policy + Understand Block Public Access at account and bucket levels. + Configure Bucket Policy to allow public access only for index.html. + Test browser access and validate security. 2025-10-08 2025-10-08 cloudjourney.awsstudygroup.com 4 Optimize performance \u0026amp; advanced security + Compare S3 + CloudFront vs S3 Transfer Acceleration vs AWS Amplify Hosting. + Enable S3 Transfer Acceleration and test with curl from multiple regions. + (Optional) Create a CloudFront distribution for HTTPS + CDN. + Monitor via CloudWatch and track costs. 2025-10-09 2025-10-09 cloudjourney.awsstudygroup.com 6 Versioning, Lifecycle \u0026amp; Replication + Resource cleanup + Enable bucket versioning, test overwrite and restore old versions. + Set lifecycle rules to transition objects to IA/Glacier. + Configure Cross-Region Replication (CRR), create IAM replication role. + Move/copy/sync objects across buckets or regions. + Clean up resources: delete test objects, disable acceleration, remove CloudFront (if any), delete test bucket. 2025-10-10 2025-10-10 cloudjourney.awsstudygroup.com Week 4 Achievements: Configuration and basic operations:\nSet up and configured AWS CLI to work with Amazon S3 (Access Key, Secret Key, default Region). Verified connection and configuration using: aws configure list aws s3 ls to list existing buckets. aws s3api list-buckets for detailed information. Created and deleted buckets via CLI: aws s3 mb s3://bucket-name aws s3 rb s3://bucket-name --force Static Website Hosting Practice:\nCreated a new bucket, uploaded index.html and error.html. Enabled Static Website Hosting and successfully accessed the website through the public endpoint. Verified 403/404 errors and ensured error.html was correctly displayed. Security and Access Management:\nEnabled and disabled Block Public Access at both account and bucket levels to understand behavior. Configured Bucket Policy to allow only index.html to be publicly accessible. Tested public/private object access for permission validation. Data Management and Performance Optimization:\nEnabled Versioning on the bucket, uploaded multiple object versions, and restored older ones. Configured Lifecycle Rules to automatically move infrequently accessed data to IA or Glacier. Enabled S3 Transfer Acceleration and measured upload speed differences across regions using curl. Advanced Operations and Data Movement:\nConfigured Cross-Region Replication (CRR) to copy objects to a different region, understood IAM Role requirements (Replication Role). Practiced CLI operations: aws s3 cp, aws s3 mv, aws s3 sync for moving and syncing data between buckets. Verified successful replication in the destination bucket. Cleanup \u0026amp; Cost Optimization:\nDeleted test objects, disabled Transfer Acceleration and CloudFront (if used). Deleted unused test buckets to prevent additional charges. Compiled best practices for cost and security: Avoid public buckets entirely. Use Versioning + Lifecycle Rules for recovery and cost reduction. Use S3 Storage Lens and CloudWatch Budgets to monitor and control costs. Notes: Never make the entire bucket public; use Bucket Policy to expose only specific objects. Enable Block Public Access at the account level by default. Use Versioning + Lifecycle Rules to prevent data loss and reduce cost. Use CloudFront or Amplify Hosting for HTTPS, CDN, and modern static site deployment. Monitor costs using CloudWatch and S3 Storage Lens. Delete carefully when Versioning is enabled—remove all versions and delete markers before deleting a bucket. Summary of Knowledge Gained: Operate Amazon S3 at a production level: hosting, security, cost optimization, versioning, and replication. Choose the right tool for static website hosting: Amplify (recommended), CloudFront + S3, or S3 only for simplicity. Perform large-scale data management: object transfer, cross-region replication, and automation with Lifecycle Rules. Clean up resources properly to avoid unexpected costs. "
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 6 Objectives: Understand the architecture of Amazon RDS and supported engines (MySQL, PostgreSQL, MariaDB, SQL Server, etc.). Create, configure, and manage an RDS Instance at the infrastructure level. Set up parameter groups, security groups, and subnet groups for RDS. Connect to RDS from EC2, Cloud9, and from a local machine. Practice backup – restore – snapshot – automated backup – failover. Become familiar with Performance Insights, Monitoring, and Enhanced Logging. Learn how to optimize cost, scale storage/compute, and clean up unused resources. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Amazon RDS Overview + Supported engines + Multi-AZ \u0026amp; Read Replica architecture + Backup/Snapshot lifecycle 13/10/2025 13/10/2025 cloudjourney.awsstudygroup.com 3 Create an RDS Instance + Create a Subnet Group + Configure Security Group (allow EC2/Cloud9) + Choose engine, instance size, storage 14/10/2025 14/10/2025 cloudjourney.awsstudygroup.com 4 Connect \u0026amp; Operate on RDS + Connect from Cloud9/EC2 + Create database \u0026amp; table + CRUD using MySQL Client or PostgreSQL Client 15/10/2025 15/10/2025 cloudjourney.awsstudygroup.com 5 Backup – Restore – Snapshot – Monitoring + Create manual snapshot + Restore from snapshot + Monitor CPU, connections + Use Performance Insights 16/10/2025 16/10/2025 cloudjourney.awsstudygroup.com 6 Scaling – Cost Optimization – Resource Cleanup + Modify instance class + Increase storage + Configure proper automated backup retention + Delete snapshots \u0026amp; RDS instances 17/10/2025 17/10/2025 cloudjourney.awsstudygroup.com Achievements in Week 6: 1. Solid understanding of Amazon RDS architecture: Differentiated between Single-AZ, Multi-AZ, and Read Replica. Understood how automated backups work (retention 1–35 days). Understood that snapshots are manual backups and not auto-deleted. 2. Successfully created and configured a full RDS Instance: Created a DB Subnet Group with 2 subnets across 2 AZs.\nCreated a Security Group allowing connections from EC2/Cloud9 (port 3306 or 5432).\nConfigured the instance with:\nEngine (MySQL/PostgreSQL) Instance class (db.t3.micro) Storage (20GB gp3) Backup retention Public/Private endpoint options 3. Successfully connected from EC2 / Cloud9 / local machine: Installed MySQL/PostgreSQL client.\nConnected using the command:\nmysql -h endpoint.amazonaws.com -u admin -p Created database, table, and performed CRUD operations:\nCREATE DATABASE demo; CREATE TABLE users (...); INSERT, SELECT, UPDATE, DELETE 4. Mastered Backup – Restore – Snapshot flow: Created manual snapshots. Restored a new RDS instance from snapshots. Observed backup windows. Verified daily automated backups. 5. Monitoring \u0026amp; Performance: Monitored CPU, RAM, and active connections from the Monitoring dashboard. Used Performance Insights to identify heavy queries. Checked Slow Query Log (if enabled). 6. Scaling \u0026amp; Cost Optimization: Modified instance class from t3.micro → t3.small. Increased storage from 20GB → 30GB. Reduced backup retention to 3 days for cost savings. Removed unnecessary snapshots. 7. Cleaned up resources to avoid extra charges: Deleted the RDS instance. Deleted subnet group. Deleted security group. Deleted old snapshots. Summary of Knowledge Gained: Understood how Amazon RDS works and common database engines. Learned how to create, configure, and connect to RDS from Cloud9 and EC2. Gained knowledge of backup, snapshot, and restore workflows. Used Performance Insights for performance monitoring. Understood how to scale and optimize RDS cost. Cleaned up resources properly to avoid unnecessary fees. "
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Part 1 – AWS CloudWatch\nUnderstand the overall architecture of Amazon CloudWatch and its components: Metrics, Logs, Alarms, Dashboard, Events. Practice creating and monitoring CloudWatch Metrics for EC2, RDS, Lambda. Collect and analyze logs using CloudWatch Logs. Create CloudWatch Alarms with custom thresholds. Build CloudWatch Dashboard to monitor the system. Practice container monitoring with CloudWatch Container Insights. Clean up resources to avoid cost. Part 2 – Hybrid DNS with Route 53 Resolver\nUnderstand hybrid DNS architecture between on-premise and AWS. Create and configure Route 53 Resolver (inbound, outbound). Create and apply Resolver Rules to forward DNS queries. Integrate on-premise DNS (Active Directory DNS) with Route 53 Private Hosted Zone. Verify bidirectional DNS resolution (AWS → on-premise, on-premise → AWS). Clean up resources: AD, endpoint, VPC configuration. Tasks to be carried out this week: Day Detailed Tasks Start Date Completion Date Documentation Source 2 CloudWatch – Overview \u0026amp; Architecture • Research CloudWatch overview and its role in AWS. • Study key components: Metrics, Logs, Alarms, Dashboards, Events, Container Insights. • Understand the difference between Basic Monitoring and Detailed Monitoring. • View end-to-end CloudWatch architecture: application → log agent → log group → metric filter → alarm → SNS → automatic action. • Create the first dashboard from EC2 default metrics. 20/10/2025 20/10/2025 cloudjourney.awsstudygroup.com 3 CloudWatch Metrics \u0026amp; Logs – Practical Implementation • Create Log Group manually and study Log Stream structure. • Install CloudWatch Agent on EC2 to collect OS-level CPU, RAM, Disk, Network data. • Configure cloudwatch-agent.json to send system logs (syslog, application log). • Create Metric Filter to detect error patterns (“ERROR”, “CRITICAL”, “Failed”). • Send custom metrics using AWS CLI (put-metric-data). • Observe retention period and test retention change. 21/10/2025 21/10/2025 cloudjourney.awsstudygroup.com 4 CloudWatch Alarm \u0026amp; Dashboard – Automated Alerts • Create CPU Alarm with 70% threshold for 5 minutes. • Create Alarm for EC2 Status Check Failure. • Connect Alarm → SNS Topic → Email for notification. • Create Alarm to automatically reboot EC2 when CPU overloads (using EC2 action). • Build custom Dashboard: add metrics widgets, logs, alarm status, text widget. • Create system-wide Dashboard monitoring EC2 + RDS + Network. • Enable Container Insights for ECS: view CPU, RAM, Task Restart. 22/10/2025 22/10/2025 cloudjourney.awsstudygroup.com 5 Route 53 Resolver – Understanding \u0026amp; Building Hybrid DNS Architecture • Study Route 53 Private Hosted Zone and DNS resolution inside VPC. • Create Outbound Resolver Endpoint for AWS → on-premise query. • Create Inbound Resolver Endpoint for on-premise → AWS query. • Create Resolver Rule forwarding “corp.local” to on-premise DNS. • Attach rules to VPC and verify endpoint status. 23/10/2025 23/10/2025 cloudjourney.awsstudygroup.com 6 Microsoft AD + DNS Integration – On-Premise DNS Simulation • Launch EC2 Windows Server and install Microsoft Active Directory. • Create domain “corp.local” and configure DNS zone in AD. • Configure Conditional Forwarder from AD → AWS Inbound Endpoint. • Create A and CNAME records in on-premise DNS. • Test bidirectional resolution: – From AWS: nslookup dc1.corp.local – From AD: nslookup api.internal.aws • Test failover when endpoint is down. 24/10/2025 24/10/2025 cloudjourney.awsstudygroup.com 7 Clean up resources, review and optimize cost • Delete unused Log Group, Log Stream, Alarm, Dashboard. • Disable Container Insights to avoid CloudWatch fee. • Delete Route 53 Resolver endpoints (hourly charges). • Delete Resolver Rules. • Disable and delete Microsoft AD domain controller. • Delete all EC2 test instances. • Check Bill → review CloudWatch Logs, metrics, AD, endpoints cost. 25/10/2025 25/10/2025 cloudjourney.awsstudygroup.com Achievements in Week 7: 1. AWS CloudWatch CloudWatch Metrics – Collection \u0026amp; Analysis\nListed all default metrics from EC2, EBS, VPC, ELB. Created custom metrics using CLI and PutMetricData. Verified 15-month retention and 1-second granularity display. Analyzed anomaly patterns: CPU Spike, abnormal Network I/O, high DiskOps. CloudWatch Logs – Application Log Collection\nInstalled CloudWatch Agent to collect OS and application logs. Created Log Group, Log Stream, pushed real-time logs. Set up log retention, exported logs to S3 for analysis. Identified common errors in logs: throttling, unauthorized, CPUCredit error. CloudWatch Alarms – Alerts and Response\nCreated alarm for CPU \u0026gt; 70% within 3 minutes. Created alarm for EC2 system status failure. Configured SNS notifications for email alerts. Enabled auto-recovery for EC2 hardware failure. CloudWatch Dashboard – Monitoring Console\nBuilt real-time monitoring dashboard. Added metrics for EC2, RDS, NAT Gateway, ELB in one interface. Integrated graph widgets, single-value widgets, and log pattern widgets. Container Insights – Microservices Monitoring\nEnabled CloudWatch Container Insights for ECS \u0026amp; Fargate. Monitored CPU/memory per task, pod, and container. Analyzed restart counts to find root causes. Evaluated how Container Insights supports fast troubleshooting. Resource Cleanup\nDeleted Dashboards, Alarms, and custom metrics. Stopped agent and removed log groups. Disabled Container Insights to reduce cost. Final Results\nMastered CloudWatch architecture and end-to-end monitoring pipeline. Successfully deployed complete monitoring workflow: metrics → logs → alarm → dashboard. Learned how to troubleshoot issues using logs and metrics. Improved skills in cost optimization and performance tuning using CloudWatch data. Gained capability to build real-time alerting systems for DevOps / SRE. 2. Building Hybrid DNS with Route 53 Resolver Understanding Hybrid DNS Architecture\nAnalyzed common issues in enterprises with on-premise DNS.\nUnderstood roles of:\nInbound Endpoint (on-premise → AWS) Outbound Endpoint (AWS → on-premise) Resolver Rules for forwarding private domains Environment Preparation\nCreated a dedicated VPC with 2 private subnets for endpoints. Created DNS Security Group with port 53 UDP/TCP. Deployed EC2 Windows Server to run Microsoft Active Directory Domain Services. Connecting through RDGW (Remote Desktop Gateway)\nConnected via RDGW to access private subnet servers. Verified DNS resolution inside VPC before hybrid configuration. Deploying Microsoft Active Directory\nConfigured on-premise Domain Controller (simulated). Created internal DNS zone: company.local. Verified SRV, A, NS records. Setting Up Hybrid DNS System\n1) Outbound Endpoint\nCreated Outbound Endpoint so EC2 in VPC can send DNS queries to on-premise AD. Assigned resolver rule forwarding company.local to on-premise DNS IPs. 2) Inbound Endpoint\nCreated Inbound Endpoint enabling on-premise DNS to query AWS Private Hosted Zone. Verified DNS resolution of Route 53 private records. 3) Resolver Rules\nCreated Forwarding rule for internal domain. Created System rule to automatically inherit VPC default DNS. Testing Full DNS Flow\nFrom on-premise server: ping Private Hosted Zone record → success. From AWS EC2: resolve on-premise domain → success. Checked DNS latency and validated two-way resolution. Resource Cleanup\nDeleted endpoints to avoid hourly charges. Deleted Private Hosted Zone and test records. Removed AD and terminated test EC2. Final Results\nFully understood enterprise hybrid DNS operation. Successfully deployed a complete Route 53 Resolver system (inbound, outbound, rules). Integrated Microsoft AD DNS with AWS DNS. Verified bidirectional DNS resolution. Gained solid knowledge of modern DNS systems for multi-cloud and hybrid-cloud environments. Summary of Knowledge Gained CloudWatch:\nEnd-to-end metrics and log collection. Intelligent alarm creation and automated responses. Building intuitive dashboards. Monitoring containers using Container Insights. Route 53 Hybrid DNS:\nUnderstanding real-world hybrid DNS architecture. Integrating on-premise DNS with Route 53 Resolver. Verifying two-way resolution. Applying architecture to enterprise deployments. "
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": " Weekly Objectives Part 1 – Amazon DynamoDB\nUnderstand DynamoDB’s NoSQL key-value \u0026amp; document architecture. Learn the core components: Partition Key, Sort Key, Partition, RCU/WCU, On-Demand, TTL, Streams, Global Tables. Create a DynamoDB table with GSI/LSI and define access patterns. Practice CRUD: PutItem, GetItem, UpdateItem, DeleteItem. Practice Query \u0026amp; Scan, FilterExpression, ConditionExpression. Use AWS CLI and JSON to interact with DynamoDB. Import a sample dataset into DynamoDB. Part 2 – Amazon ElastiCache for Redis\nUnderstand in-memory caching structure, cluster mode enabled/disabled. Create a Redis Cluster inside a VPC: node, parameter group, subnet group. Connect to Redis from EC2 via redis-cli. Test data operations: SET, GET, TTL \u0026amp; persistence snapshot. Apply the Cache-Aside model between Redis \u0026amp; DynamoDB. Measure cache-hit vs cache-miss performance. Clean up resources to avoid unexpected costs. Tasks to be carried out this week: Day Detailed Tasks Start Date Completion Date Reference 2 DynamoDB – Overview \u0026amp; Architecture • Learn that DynamoDB is a key-value/document NoSQL database. • Understand partitions and how data is distributed via Partition Key. • Understand WCU/RCU, Auto Scaling, On-Demand. • Learn roles of GSI/LSI and common access patterns. • Learn about TTL \u0026amp; Streams. 27/10/2025 27/10/2025 cloudjourney.awsstudygroup.com 3 Create DynamoDB Table \u0026amp; Real Query Tests • Create a table with Partition Key + Sort Key. • Create GSIs to expand query flexibility. • Enable TTL and test record expiration. • Practice CRUD (Put/Get/Update/Delete). • Practice Query vs Scan, FilterExpression, ProjectionExpression. 28/10/2025 28/10/2025 cloudjourney.awsstudygroup.com 4 DynamoDB via AWS CLI • Create DynamoDB tables using CLI. • Import sample JSON file. • Query via CLI: KeyConditionExpression. • UpdateItem with ConditionExpression. • Observe throughput \u0026amp; consumed capacity. 29/10/2025 29/10/2025 cloudjourney.awsstudygroup.com 5 ElastiCache Redis – Architecture \u0026amp; Deployment • Understand in-memory architecture, replication group. • Create subnet group \u0026amp; security group for Redis. • Deploy Redis Cluster (1 primary + replicas). • Enable automatic failover. • Connect EC2 to Redis via redis-cli. 30/10/2025 30/10/2025 cloudjourney.awsstudygroup.com 6 Advanced Redis Practice \u0026amp; DynamoDB Integration • Test GET, SET, EXPIRE, TTL. • Test persistence snapshot (RDB). • Build Cache-Aside model: – Cache hit returns immediately. – Cache miss → read from DynamoDB. – Write back to Redis cache. • Measure Redis vs DynamoDB latency. 31/10/2025 31/10/2025 cloudjourney.awsstudygroup.com 7 Resource Cleanup \u0026amp; Performance Evaluation • Delete Redis Cluster, subnet group, parameter group. • Delete test DynamoDB tables. • Check Billing for WCU/RCU usage. • Evaluate performance difference: Redis: ~1–2ms vs DynamoDB: ~10–20ms. 1/11/2025 1/11/2025 cloudjourney.awsstudygroup.com Achievements in Week 8 : 1. Amazon DynamoDB Architecture \u0026amp; Table Design\nFully understand partitioning, scaling, and throughput mechanism. Learn how to design Partition Key and Sort Key to avoid hotspots. Optimize access patterns using GSI \u0026amp; LSI. CRUD \u0026amp; Query\nSuccessfully executed CRUD operations using both Console \u0026amp; CLI. Proficient with Query using KeyConditionExpression. Understand FilterExpression, UpdateExpression. Performed table Scan and advanced filtering. Advanced DynamoDB Management\nEnabled TTL \u0026amp; validated automatic item deletion. Explored DynamoDB Streams \u0026amp; event-driven architecture. Evaluated cost based on RCU/WCU vs On-Demand. 2. Amazon ElastiCache for Redis Redis Cluster Deployment\nCreated Redis inside VPC with correct subnet group \u0026amp; security group. Configured replication group + automatic failover. Successfully connected to Redis from EC2. Redis Operations\nProficient with SET, GET, DEL, TTL, EXPIRE. Tested RDB persistence snapshots. Analyzed eviction policies when cache is full. 3. DynamoDB + Redis Integration (Cache-Aside) Built workflow: check cache → query DB → update cache. Tested real-world cache hit/miss scenarios. Measured latency: Redis is ~10× faster than DynamoDB. Understood why Redis caching is essential for read-heavy systems. Learned real-world use cases: user profile caching, session store, product caching. Summary of Knowledge Gained DynamoDB:\nDistributed NoSQL architecture. Partition Key, Sort Key, GSI/LSI. CRUD, Query, Scan. RCU/WCU Throughput \u0026amp; On-Demand mode. TTL, Streams. ElastiCache Redis:\nUltra-fast in-memory architecture. Cluster mode, replication, failover. TTL, persistence snapshot. Combining DynamoDB + Redis:\nCache-Aside pattern. Cache hit/miss \u0026amp; performance benefits. Practical usage in microservices architectures. "
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Understanding and Configuring Amazon VPC\nWeek 3: Deploying and Managing Amazon EC2 Instances on Windows\nWeek 4: Implementing IAM Roles and Working with AWS Cloud9\nWeek 5: Hosting and Managing Static Websites with Amazon S3\nWeek 6: Working with Amazon RDS – Creating, Configuring, Connecting \u0026amp; Operating Databases on AWS\nWeek 7: AWS CloudWatch \u0026amp; Hybrid DNS với Route 53 Resolver\nWeek 8: Working With AWS DynamoDB \u0026amp; Amazon ElastiCache\nWeek 9: Optimizing EC2 Costs with Lambda\nWeek 10: DMS - Introduction DMS and Writing Lambda Functions\nWeek 11: Serverless - Using Amplify Authentication and Storage for Serverless Applications \u0026amp; Serverless - A Guide to Writing Front-End Code for Calling API Gateway\nWeek 12: AI Agent with Amazon Bedrock on AWS\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.9-week9/",
	"title": "Worklog Week 9",
	"tags": [],
	"description": "",
	"content": " Weekly Objectives Part 1 – EC2 Cost Optimization with Lambda Function\nUnderstand the automated Start/Stop EC2 model for cost optimization. Create tags to identify EC2 instances that need to be automatically started/stopped. Write a Lambda Function to automatically Start/Stop EC2 based on tags. Integrate EventBridge Scheduler to trigger Lambda on schedule. Validate actual behavior through CloudWatch Logs. Clean up resources after testing. Part 2 – EC2 Cost Optimization with Savings Plan\nUnderstand what Savings Plans are and how they reduce costs. Distinguish between Compute Savings Plans and EC2 Instance Savings Plans. Understand USD/hour commit model and how the estimator calculates commitments. Learn the proper AWS process for purchasing a Savings Plan. Calculate cost using AWS Pricing Calculator. Tasks to be carried out this week: Day Detailed Tasks Start Date Completion Date Reference 2 Introduction \u0026amp; analysis of the EC2 cost optimization model\n• Understand the wastefulness when EC2 runs 24/7 but is used only for a few hours/day.\n• Analyze the model: User → EventBridge → Lambda → EC2 API.\n• Categorize workloads suitable for Auto Start/Stop.\n• Explore risks and limitations of stopping/starting EC2. 03/11 03/11 cloudjourney.awsstudygroup.com 3 Create tags to identify instances for automation\n• Create Tag Key: Schedule.\n• Create Tag Values: office-hours, training-only, weekend-shutdown.\n• Apply tags to the EC2 instances that need cost optimization.\n• Validate via AWS CLI: describe-instances --filters tag:Schedule=office-hours. 04/11 04/11 cloudjourney.awsstudygroup.com 4 Create IAM Role for Lambda Function\n• Create role name: LambdaEC2ScheduleRole.\n• Attach policy: AmazonEC2FullAccess (for training).\n• Attach logging permission: AWSLambdaBasicExecutionRole.\n• Verify trust relationship: lambda.amazonaws.com. 05/11 05/11 cloudjourney.awsstudygroup.com 5 Create Lambda Function for Start/Stop EC2\n• Write Python code using boto3: StartInstances \u0026amp; StopInstances.\n• Logic to filter EC2 by \u0026lt;Schedule\u0026gt; tag.\n• Log instance IDs that were started/stopped for verification.\n• Manually test using “Test event”. 06/11 06/11 cloudjourney.awsstudygroup.com 6 Create EventBridge Scheduler for automation\n• Create a rule to start EC2 at 8:00 AM.\n• Create a rule to stop EC2 at 18:00 PM.\n• Assign Lambda Function as the target.\n• Check actual logs in CloudWatch Logs. 07/11 07/11 cloudjourney.awsstudygroup.com 7 Validate results \u0026amp; clean up resources\n• Test if EC2 starts/stops correctly on schedule.\n• Check CloudWatch Logs to debug permission errors.\n• Remove EventBridge Rules, Lambda Function, IAM Role after completion.\n• Review EC2 cost before and after optimization. 08/11 08/11 cloudjourney.awsstudygroup.com Achievements in Week 9 : 1. EC2 Cost Optimization with Lambda\nSuccessfully implemented automatic EC2 start/stop according to business hours. Understood and used Tags to group EC2 instances for cost optimization. Wrote Lambda Function to automatically Start/Stop EC2 using boto3. Automated the process using EventBridge Scheduler on a fixed schedule. Generated detailed logs: started/stopped instance IDs, execution time, permission errors. Reduced 50–70% EC2 costs for workloads operating only a few hours per day. 2. Cost Optimization with Savings Plans\nFully understood that Savings Plans operate based on USD/hour commitment.\nComputed appropriate commitment amount using AWS Pricing Calculator.\nDifferentiated between the two Savings Plan types:\nCompute Savings Plan → most flexible. EC2 Instance Savings Plan → highest discount. Learned the correct process for purchasing and applying Savings Plans.\nUnderstood benefits: up to 66% cost savings when applied correctly.\nSummary of Knowledge Gained AWS Lambda + EC2 Automation\nAutomated EC2 start/stop using Lambda. Used Boto3 to call EC2 control APIs. EventBridge Scheduler used for periodic triggers. CloudWatch Logs used for analysis and debugging. AWS Savings Plans\nMethod of committing costs to reduce AWS infrastructure expenses. Best suited for workloads that run 24/7. Differences between Compute vs. EC2 Instance Savings Plans. Combining Auto Scheduling + Savings Plans for maximum optimization. "
},
{
	"uri": "http://localhost:1313/aws-workshop/5-workshop/5.4-s3-onprem/5.4.2-create-agent/",
	"title": "Create Agent",
	"tags": [],
	"description": "",
	"content": " On the interface page of Aws Bedrock, I choose the left side Agent Please fill in the name and architecture for the Agent to help it understand the context For example, I write it as an Aws consultant. But before I can get that answer, I have to load Knowledge base for it to understand - Content of Knowledge base Note that you should configure it according to your needs and should grant it the necessary permissions\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "1. Create an IAM Role using the Console with the required permissions Create an IAM role named bedrock and assign the necessary permissions to use Amazon Bedrock: model/agent invocation permissions (bedrock:InvokeModel, bedrock:CreateAgent, bedrock:InvokeAgent), S3 access permissions (read/write) to load documents to the Knowledge Base, and CloudWatch Logs permissions to write logs, etc.\nDocument templates to load kwoledge base (PDF, text, markdown, HTML) We will create an introduction document to Amazon Web Services (Text) Amazon Web Services (AWS) is Amazon\u0026#39;s cloud computing platform, officially launched in 2006. Initially, AWS was created to solve Amazon\u0026#39;s internal problem: they needed a flexible scalable infrastructure to serve their huge e-commerce system. When realizing that other businesses were also facing similar difficulties with IT infrastructure, Amazon decided to commercialize this platform and provide it as a cloud service. AWS was born with the goal of helping organizations and businesses reduce server investment costs, deploy applications faster, and take advantage of infrastructure on demand instead of having to operate data centers themselves. This is the core vision behind the modern \u0026#34;cloud computing\u0026#34; model: turning computing resources into a utility service that can be used immediately, like electricity and water. In the following years, AWS grew rapidly, from a few basic services such as storage (Amazon S3) and virtual machines (EC2) to an ecosystem of more than 200 services including virtual servers, containers, artificial intelligence, data storage, security, IoT and data analytics. Thanks to its global scalability with dozens of Regions and hundreds of Edge points around the world, AWS became the leading cloud platform in the market. AWS\u0026#39;s long-term goal is to provide a secure, flexible computing environment that can meet the needs of all users — from small startups to large corporations. AWS aims to help businesses focus on product development, instead of spending time operating infrastructure. To date, AWS is used by leading technology companies, governments, universities and millions of organizations around the world. AWS is not just an infrastructure service, but has also become an important platform to drive innovation in areas such as machine learning, Big Data, generative AI with (Amazon Bedrock), and serverless application development. Thanks to that, AWS plays an important role in shaping the modern technology generation. "
},
{
	"uri": "http://localhost:1313/aws-workshop/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "FoodMind Recommender Platform For Prompt-IPoG An integrated AWS Serverless solution for personalized meal tracking and recommendations. 1. Executive Summary FoodMind Recommender Platform is an intelligent web platform designed to serve as a personalized eating assistant. It automatically calculates the user\u0026rsquo;s Total Daily Energy Expenditure (TDEE) based on their profile and leverages AWS Bedrock (Generative AI) to allow users to log meals in natural language (e.g., “I just ate a bowl of beef pho”).\nThe system also provides intelligent meal recommendations (Breakfast/Lunch/Dinner) by automatically filtering dishes according to calorie goals and health constraints (e.g., “allergies”, “gout”).\nThe entire solution is built on a serverless architecture with AWS Amplify (Frontend), API Gateway, AWS Lambda, and Amazon DynamoDB (Backend). It enables users to receive meal recommendations based on their calculated calorie intake for the day. Data is stored and queried through Amazon DynamoDB, ensuring high performance and flexible scalability.\nThe solution emphasizes the combination of AI and real-world data to support smart meal decision-making and offers a dashboard for users to track their eating habits effectively.\n2. Problem Statement Current Problem\nMany users struggle to manage their daily diet — they don’t know how many calories to consume or which foods suit their daily health goals. Manual logging and nutritional lookup are time-consuming, inaccurate, and not personalized.\nSolution\nFoodMind Recommender Platform applies AI and AWS Cloud to automate the entire meal tracking and recommendation process:\nGoal Automation: Automatically calculates calorie targets (using the Mifflin-St Jeor formula) when users update their profiles.\nRecommendation Automation: Provides a GET /recommendations API using Lambda business logic to filter dishes from the database based on calorie targets (e.g., lunch \u0026lt; 700 calories) and health restrictions (e.g., avoid “red meat” for gout).\nAI Logging Automation: Provides a POST /log-food API using AWS Bedrock to parse natural language input, extract calories, and store logs.\nSelf-learning Automation: When users log a new dish (e.g., “bun dau mam tom”) unknown to the system, AWS Bedrock estimates its calories and automatically adds it to the knowledge base.\nVisual Tracking: The dashboard (Amplify) displays a 7-day meal history, helping users monitor and manage their dietary habits.\nUsers simply input data — the system understands, analyzes, and recommends suitable meals tailored to personal health goals.\nBenefits and ROI\nSaves time on nutrition tracking, eliminates manual work. Offers highly personalized AI experience. Creates a structured dataset for AI research in food and nutrition. Low cost thanks to serverless architecture. Scalable and reusable for other health-related applications. ROI estimate: payback within 6 months through reduced development time and model reuse. Estimated cost: around 10–15 USD/month. 3. Solution Architecture The platform is fully built on AI-as-a-Service combined with AWS Serverless, ensuring high performance, security, and scalability. Nutritional data on dishes is stored in Amazon DynamoDB, which supports generating meal recommendations based on calculated calorie targets. Amazon Bedrock processes user natural language to extract calorie information and log meals in DynamoDB. AWS Amplify hosts the Next.js web interface, and Amazon Cognito ensures secure user authentication. The architecture is detailed below:\nAWS Services Used\nAWS Amplify: Deploys and hosts the web interface (Next.js), integrates with GitLab CI/CD for auto build and deploy. Amazon Route 53 + AWS WAF + Amazon CloudFront: Edge layer for secure and fast content delivery worldwide. Amazon Cognito: Manages user authentication, login, and access control. Amazon API Gateway: Provides endpoints for GET /Recommendation, POST /Log, GET /Dashboard, connected to Lambda. AWS Lambda (Private Subnet): Handles business logic, calls Bedrock and DynamoDB via VPC Endpoints for security. AWS Bedrock: Generates dish descriptions, normalizes meal logs, and stores them in DynamoDB for personalized recommendations. Amazon DynamoDB: Stores user data, meal logs, calorie goals, and recommendation data with scalable performance. AWS Secrets Manager: Secures credentials (API Keys, Bedrock access) for Lambda and backend. Amazon CloudWatch \u0026amp; AWS CloudTrail: Monitors logs, access, and performance; supports incident recovery. Amazon S3: Stores system logs and backups. AWS IAM: Manages detailed access permissions between services and users. Amazon VPC: Isolates Lambda in a private subnet to ensure secure internal communication between Lambda, DynamoDB, and Bedrock. Component Design\nUser Management: Amazon Cognito controls user access. Content Delivery \u0026amp; Security: Route 53 routes domain, WAF prevents web attacks (SQL Injection, DDoS), CloudFront speeds up global delivery. Web Interface: Amplify hosts the Next.js app. Meal Logging \u0026amp; Recommendation: User input (text) stored in DynamoDB; Lambda recommends dishes based on calorie calculations. Meal Analysis: Lambda calls Bedrock to process user input, extract calorie data, and log new dishes in DynamoDB if missing. Dashboard Display: Amplify shows calorie charts by day/week/month. Authentication \u0026amp; Security: Cognito ensures secure login and user management. Monitoring \u0026amp; Tracking: CloudWatch monitors logs and Lambda performance; CloudTrail records API and user activity history. 4. Technical Implementation Implementation Phases\nResearch \u0026amp; Design: Build AI + Cloud pipeline, check feasibility, and design AWS architecture (Weeks 1–5). Cost Optimization: Validate AWS service pricing to optimize budget (Week 6). Development \u0026amp; Deployment: Load initial data, build Next.js frontend, test APIs, and deploy final product (Weeks 7–11). Technical Requirements\nCalorie Dataset: Collect initial data and load into DynamoDB using AWS SDK (Boto3). Recommendation Platform: Requires knowledge of AWS Amplify (Next.js hosting), S3, Cognito, and Serverless stack (Lambda, DynamoDB, API Gateway), DynamoDB schema (PK, SK), and Bedrock API integration. 5. Roadmap \u0026amp; Milestones Internship (Jan–Mar): January: Learn and master AWS services. February: Design and refine architecture. March: Deploy, test, and launch the system. Post-deployment: Maintain, enhance recommendations, and expand data within one year. 6. Budget Estimation Cost reference: AWS Pricing Calculator\nOr download budget estimation file.\nInfrastructure Cost\nAWS Amplify: 0.50 USD/month (~100 MB, low traffic). AWS Lambda: 0.20–0.30 USD/month (100,000 requests/month, avg runtime \u0026lt;1s). Amazon API Gateway: 0.10–0.20 USD/month (50,000 REST API requests/month). Amazon DynamoDB (Paid Plan): ~0.30 USD/month (50 MB data, ~20,000 requests/month, On-Demand). Amazon S3 (log/backup): 0.10 USD/month (\u0026lt;2GB). AWS Bedrock: 3.00–5.00 USD/month (a few thousand tokens/month). CloudWatch + CloudTrail + IAM: ~0.10 USD/month. Amazon Cognito: 0.00 USD/month (\u0026lt;50 active users, Free Tier). Total: ~4–6 USD/month (~50–75 USD/year).\n7. Risk Assessment Risk Matrix\nAI misinterpretation — High impact, Low probability. API overload — Medium impact, Low probability. Budget overrun — Medium impact, Low probability. Logic error — Medium impact, Low probability. Mitigation Strategies\nAI deviation: Careful prompt engineering. API overload: Request throttling via API Gateway. Budget: AWS budget alerts and service optimization. Logic: Rigorous Lambda testing and validation. Contingency Plan\nManual data collection fallback if AWS outage occurs. Use CloudFormation to restore infrastructure configurations. 8. Expected Outcomes Enhanced User Experience: Provides a smart meal assistant, removing manual effort in tracking and choosing meals.\nPractical AI Integration: Demonstrates real-world AWS Bedrock integration in production-level systems.\nNutrition Data Foundation: Expandable dataset for AI and healthcare research.\nScalability: Extendable to image-based food analysis, AI chat coaching, and mobile applications.\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": " Weekly Objectives Part 1 – Build the backend for the Document Management System (DMS) using Lambda + DynamoDB\nUnderstand the overall architecture of the Document Management System. Design the DynamoDB table according to real-world access patterns. Build Lambda functions for document CRUD operations (Create, Read, Query, Delete). Configure IAM Roles for Lambda following the principle of least privilege. Test APIs using test events and CloudWatch Logs. Prepare the foundation for integrating API Gateway and Amplify in the next week. Part 2 – Understand how the DMS will scale in upcoming weeks\nUnderstand how Cognito Authentication and Amplify Storage will be integrated. Identify the role of DynamoDB Streams for the search engine using OpenSearch. Understand the process of deploying the entire backend using AWS SAM. Understand the expected CI/CD pipeline using AWS CodePipeline. Tasks to be carried out this week: Day Detailed Tasks Start End Source 2 Analyze the overall DMS architecture\n• Understand the application goals: upload, view, download, delete, and search documents.\n• Identify main components: Lambda, DynamoDB, S3, Cognito, API Gateway.\n• Analyze workflow: Upload → Save metadata → Search processing.\n• List all required backend API endpoints. 11/10 11/10 cloudjourney.awsstudygroup.com 3 Design DynamoDB Table for Document Metadata\n• Create table Documents with PK = userId, SK = documentId.\n• Design attributes: filename, fileType, size, tags, createdAt, updatedAt.\n• Analyze access patterns: Query by user, Get by documentId, filter by tag.\n• Prepare sample data and test via AWS Console + CLI. 11/11 11/11 cloudjourney.awsstudygroup.com 4 Create IAM Role for Lambda Functions\n• Create role DMSLambdaRole.\n• Grant DynamoDB CRUD permissions: GetItem, Query, PutItem, DeleteItem.\n• Grant CloudWatch Logs permissions.\n• Validate Trust Policy: lambda.amazonaws.com.\n• Test assume role using Lambda test environment. 11/12 11/12 cloudjourney.awsstudygroup.com 5 Develop CreateDocument Lambda Function\n• Validate user input.\n• Auto-generate documentId using UUID.\n• Save metadata into DynamoDB using put_item.\n• Add input/output logging for monitoring.\n• Test using Lambda test events + CloudWatch Logs.\n• Improve exception handling: missing fields, DynamoDB errors. 11/13 11/13 cloudjourney.awsstudygroup.com 6 Develop GetDocuments \u0026amp; GetDocumentById Lambda Functions\n• Implement query to fetch all documents by userId.\n• Support pagination using LastEvaluatedKey.\n• Implement GetDocumentById to fetch single document details.\n• Return structured JSON for frontend use.\n• Test using sample DynamoDB data. 11/14 11/14 cloudjourney.awsstudygroup.com 7 Develop DeleteDocument Lambda Function \u0026amp; Cleanup\n• Verify document existence before deletion.\n• Delete metadata using delete_item.\n• Prepare logic for deleting real files from S3 (scheduled for next week).\n• Log the delete process for observability.\n• Clean up test resources and remove unnecessary records. 11/15 11/15 cloudjourney.awsstudygroup.com Achievements in Week 10 : 1. Completed DynamoDB table with optimal design\nFully built the Documents table meeting the needs of a Document Management System. Designed PK/SK optimized for querying by user and retrieving detailed items. Prepared a scalable data model for future integration with OpenSearch or DynamoDB Streams. 2. Successfully developed all Lambda CRUD functions\nCreateDocument – create new documents. GetDocuments – query documents by userId. GetDocumentById – retrieve a document by documentId. DeleteDocument – delete documents safely. All functions follow Serverless best practices with full logging and safe retries. 3. Applied best practices for security and logging\nIAM Roles strictly follow the least privilege principle. CloudWatch Logs used to trace all execution flows. Clear error handling: input validation, DynamoDB errors, missing items. 4. Fully understood the DMS architecture and prepared for next week\nMastered the workflow: Authentication → API → Storage → Metadata. Understood how Lambda will integrate with API Gateway in Week 11. Built a solid technical foundation for next week’s Amplify Storage + Authentication implementation. Summary of Knowledge Gained AWS DynamoDB\nDesigning tables based on access patterns. Proper use of Partition/Sort keys. Query, PutItem, GetItem, DeleteItem with Boto3. AWS Lambda\nWriting serverless CRUD functions. Creating optimized IAM Roles + policies. Logging using CloudWatch Logs. Production-grade error handling. DMS Architecture\nMetadata stored in DynamoDB. Actual files stored in S3. Authentication handled by Cognito. APIs orchestrated through Lambda + API Gateway. "
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " Weekly Objectives Part 1 – Serverless: Using Amplify Authentication \u0026amp; Storage\nUnderstand how Amplify Libraries, Amplify Auth, and Amplify Storage work. Integrate Cognito Authentication into a web application. Build a file upload system from the frontend and store files in S3 using Amplify Storage. Learn the different access levels (public / protected / private). Understand the overall architecture involving Cognito + S3 + Amplify. Part 2 – Serverless: Building a Front-end that Calls API Gateway\nUnderstand the workflow of creating an API Gateway connected to Lambda. Build a frontend using JavaScript/React to call API Gateway → Lambda → DynamoDB. Test APIs using Postman and direct frontend requests. Understand the architecture from front-end → API Gateway → Lambda → DynamoDB. Tasks to be carried out this week: Day Detailed Tasks Start Finish Source Mon Introduction to Amplify \u0026amp; Environment Setup\n• Learn Amplify Libraries, CLI, and UI Components.\n• Initialize a web project for Amplify integration.\n• Install @aws-amplify/ui and aws-amplify packages.\n• Configure AWS environment. 17/11 17/11 cloudjourney.awsstudygroup.com Tue Authentication with Amplify Auth (Cognito)\n• Create User Pool \u0026amp; Identity Pool.\n• Configure Amplify Auth in the frontend.\n• Build login UI via Amplify UI Components.\n• Test signup, login, password reset. 18/11 18/11 cloudjourney.awsstudygroup.com Wed File Storage with Amplify Storage (S3)\n• Create S3 bucket using Amplify.\n• Configure public/protected/private access levels.\n• Implement file upload from frontend → S3.\n• Verify file on S3 console.\n• Implement list files \u0026amp; get URL functions. 19/11 19/11 cloudjourney.awsstudygroup.com Thu Deploy API Gateway connected to Lambda\n• Create REST API.\n• Map methods to Lambda CRUD functions.\n• Enable CORS for frontend.\n• Deploy updated Lambda ARN. 20/11 20/11 cloudjourney.awsstudygroup.com Fri API Testing with Postman\n• Send GET/POST/DELETE requests.\n• Include Cognito Authorization token.\n• Check logs in CloudWatch.\n• Fix CORS and IAM permission issues. 21/11 21/11 cloudjourney.awsstudygroup.com Sat Integrate Frontend with API Gateway\n• Create JavaScript service calling API Gateway.\n• Attach Cognito token to request headers.\n• Render data returned from Lambda/DynamoDB.\n• End-to-end test: Frontend → API → Lambda → DynamoDB.\n• Cleanup test resources. 22/11 22/11 cloudjourney.awsstudygroup.com Achievements in Week 11 : 1. Completed Authentication using Amplify + Cognito\nUser signup/login flows fully implemented. Support for verification, login, password reset flows. Frontend successfully retrieves JWT token for API Gateway calls. Strong understanding of User Pool and Identity Pool. 2. File Upload \u0026amp; Management using Amplify Storage + S3\nSuccessfully uploaded files from frontend.\nS3 bucket automatically created by Amplify.\nVerified all 3 access levels:\npublic protected private Implemented:\nupload list files get URL delete file 3. Built a Complete API Gateway for the Document Management System (DMS)\nCreated REST API with CRUD routes. Integrated Lambda functions from previous week. Fully enabled CORS. Tested all endpoints using Postman with Cognito JWT tokens. 4. Frontend Integration with API Gateway\nImplemented request logic including Cognito tokens. Displayed documents retrieved from DynamoDB. Successfully executed full workflow: Login → Upload File → Save Metadata → Query → Display Data\nSummary of Knowledge Gained Amplify\nAmplify Auth uses Cognito as backend. Amplify Storage simplifies S3 operations. Amplify UI provides quick authentication components. Cognito (Authentication)\nUser Pool manages users. Identity Pool provides temporary credentials for S3. JWT tokens used for API Gateway authentication. S3 (Storage)\nObjects organized by access levels. Direct frontend uploads via Amplify. API Gateway\nRoutes requests to Lambda. CORS configuration is crucial for frontend. Can enforce Cognito token authentication. Lambda + DynamoDB\nLambda handles CRUD logic. DynamoDB stores document metadata. API Gateway enables frontend queries through Lambda. "
},
{
	"uri": "http://localhost:1313/aws-workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "-\nWeekly Objectives Part 1 – Introduction to Amazon Bedrock \u0026amp; Foundation Models\nUnderstand Amazon Bedrock architecture and key services. Learn how to access and use foundation models (FM) via Bedrock. Explore model options: Claude, Llama, Mistral, Amazon Titan\u0026hellip; Learn the difference between text generation, embeddings, and multimodal models. Part 2 – Building an AI Agent with Bedrock Agent\nUnderstand what a Bedrock Agent is and how it works.\nLearn the concept of:\nAction groups Knowledge bases Orchestration Lambda function integration Configure an AI Agent capable of calling external tools via Lambda.\nPart 3 – Using Knowledge Bases for Enterprise Search\nCreate a Knowledge Base with vector embeddings. Connect Bedrock KB to S3 documents. Test semantic search and RAG (Retrieval Augmented Generation). Integrate the KB into the Bedrock Agent. Part 4 – Front-end Integration with Bedrock Agent\nBuild a web UI chat application to communicate with the Bedrock Agent. Implement streaming responses. Pass user input → Agent → (KB, Lambda Tools) → Response. Deploy frontend using Amplify Hosting. Tasks to be carried out this week: Day Detailed Tasks Start Finish Source Mon Learn Fundamentals of Amazon Bedrock\n• Explore model catalog.\n• Test Claude / Llama 3 in Playground.\n• Compare pricing \u0026amp; capabilities.\n• Learn Bedrock API basics. 24/11 24/11 cloudjourney.awsstudygroup.com Tue Build First Bedrock Text Generation App\n• Write Lambda calling Bedrock InvokeModel API.\n• Test temperature, max token params.\n• Build small CLI app using SDK. 25/11 25/11 cloudjourney.awsstudygroup.com Wed Create Bedrock Agent\n• Set up agent configuration.\n• Add instructions \u0026amp; guardrails.\n• Create Action Group backed by Lambda.\n• Connect agent to DynamoDB test table. 26/11 26/11 cloudjourney.awsstudygroup.com Thu Build Knowledge Base (RAG System)\n• Create S3 bucket for documents.\n• Configure embeddings (Titan Embeddings G1).\n• Test semantic Q\u0026amp;A with PDF/CSV files.\n• Integrate KB into Agent. 27/11 27/11 cloudjourney.awsstudygroup.com Fri End-to-End AI Agent Testing\n• Chat: Agent → KB → Lambda → Return Answer.\n• Fix permission issues (IAM, kms).\n• Test multi-turn conversation.\n• Add fallback and error handling. 28/11 28/11 cloudjourney.awsstudygroup.com Sat Frontend Chat App using Amplify\n• Build React chat UI.\n• Connect UI to Bedrock Agent API via API Gateway.\n• Implement streaming output.\n• Deploy using Amplify Hosting. 29/11 29/11 cloudjourney.awsstudygroup.com Achievements in Week 12 : 1. Strong Understanding of Amazon Bedrock \u0026amp; Foundation Models\nLearned how to call models using the SDK. Tested multiple model families. Understood use cases: text generation, summarization, code, embeddings. 2. Built a Fully Functional AI Agent\nAgent can understand user intent. Uses Action Groups to execute Lambda functions. Supports multi-step reasoning and orchestration. Agent can retrieve data from DynamoDB via Lambda. 3. Implemented Knowledge Base with RAG\nCreated a vector store from S3 documents. Successfully ran semantic search and contextual Q\u0026amp;A. Integrated KB into the agent → more accurate answers. 4. Created a Web Chat Interface Connected to Bedrock\nBuilt a React chat interface. Integrated API Gateway for secure communication. Implemented real-time response streaming. Successfully connected UI → Agent → KB → Lambda → Response. Summary of Knowledge Gained Amazon Bedrock\nProvides secure access to leading foundation models. Fully managed, scalable, and enterprise-ready. Supports text, embeddings, and multimodal models. Bedrock Agents\nAI Agent can hold conversation, manage context, and call tools. Action groups allow integration with real backend systems. Guardrails help enforce safety \u0026amp; content filtering. Knowledge Bases\nVector embeddings enable semantic search. KB + Agent = RAG-powered conversational system. Automatic sync with S3 documents. Integration \u0026amp; Deployment\nLambda for executing backend logic. API Gateway for secure access to frontend. Amplify for hosting the chat application. "
},
{
	"uri": "http://localhost:1313/aws-workshop/5-workshop/5.3-vpc-to-internet/",
	"title": "Exploring Bedrock Console",
	"tags": [],
	"description": "",
	"content": " In this section, you will explore Amazon Bedrock Console and run quick tests in Playground to evaluate both the quality and cost of your models. Before selecting a model for production, measure the average token count, compare responses between models, and estimate costs to choose the model that fits your budget. Contents Exploring Bedrock "
},
{
	"uri": "http://localhost:1313/aws-workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section will list and introduce the blogs you have translated. For example:\nModernizing Java Applications with Amazon Q Developer and Visual Studio Code This blog covers the process of modernizing legacy Java applications using Amazon Q Developer and Visual Studio Code. It covers configuring a development environment with multiple JDK versions, integrating Amazon Q Developer, and using the /transform feature to automatically migrate applications from Java 8 or Java 11 to Java 21. It also describes the ability to generate automated unit tests using /test, which can help reduce technical debt, optimize performance, and simplify maintenance of Java applications.\nModernizing trading workloads with next-generation AWS Outposts racks This blog introduces how to modernize financial trading systems with next-generation AWS Outposts, which leverage Amazon EC2 instances with accelerated networking to achieve slow speeds and deterministic performance. It covers key capabilities such as bare metal compute, multicast, concurrent PTP, and Outposts deployment models that ensure complementarity, reliability, and adjacent connectivity to AWS.\nModify Amazon EBS volumes on Kubernetes with Volume Attributes Classes This blog explains how to modify Amazon EBS volumes in Kubernetes using the new VolumeAttributesClass (VAC) API. It shows how Kubernetes users can change EBS volume properties—such as type, IOPS, throughput, and AWS tags—without downtime. The post also demonstrates practical workflows, including upgrading gp2→gp3 for cost savings, increasing performance, and automating backups using AWS Data Lifecycle Manager. It provides step-by-step examples using PersistentVolumeClaims, StorageClasses, and the Amazon EBS CSI driver.\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/5-workshop/5.4-s3-onprem/",
	"title": "Agent Design",
	"tags": [],
	"description": "",
	"content": "Overview In this section, we will learn how to design an AI Agent that operates on the Amazon Bedrock platform. The main goal of the chapter is to help you understand what an Agent is, how it handles user requests, and why Agents play an important role in building modern AI applications.\nIn this overview, we will outline the important components of the Agent design process, including how to define the Agent\u0026rsquo;s tasks, describe the desired behavior, structure the workflow, choose the Foundation Model, as well as the principles to follow for the Agent to operate stably and accurately. This section also introduces how Agents interact with enterprise data and integrate with AWS services for deployment in production environments.\nContents Create Knowledge base Create Agent "
},
{
	"uri": "http://localhost:1313/aws-workshop/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Cloud Day\nDate \u0026amp; Time: 9:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Generative AI with Amazon Bedrock\nDate \u0026amp; Time: 08:30, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Building an AI Agent with AWS Bedrock Overview Amazon Bedrock is a fully managed AI/ML service provided by AWS, allowing businesses to easily access and utilize foundational models such as Claude, Llama, Stable Diffusion, Amazon Titan, and others without the need to build complex AI infrastructure.\nWe can call these models through the Bedrock API or Bedrock console without having to deploy AI infrastructure ourselves.\nContent Workshop Overview Preparing the AWS Environment Building a Basic AI Agent Creating API Gateway for the AI Agent Testing and Deployment Cleaning Up Resources "
},
{
	"uri": "http://localhost:1313/aws-workshop/5-workshop/5.6-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "Cleanup Resources Congratulations on completing this workshop!\nIn this workshop, you learned about AI Agent with bedrock on Aws and tried it out with python (boto3).\nCleanup Navigate to AWS Bedrock on the left side of the dashboard. Select Agent. Delete and confirm the deletion by typing \u0026ldquo;delete\u0026rdquo;. Navigate to AWS Bedrock on the left side of the dashboard. Select Knowledge Base .Delete and confirm the deletion by entering the keyword \u0026ldquo;delete\u0026rdquo; Delete the unused IAM Role "
},
{
	"uri": "http://localhost:1313/aws-workshop/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services (AWS) from December 8 to December 12, I had a valuable opportunity to learn, practice and apply the knowledge I acquired in the classroom to a professional working environment.\nI participated in **deep learning about Amazon Bedrock service and AWS AI ecosystem, as well as many Aws services, thereby significantly improving my skills in requirements analysis, system architecture thinking and working with technical documents.\nIn terms of style, I always try to complete assigned tasks well, comply with AWS\u0026rsquo;s rules and culture, and actively exchange and learn from my mentor and friends to improve work efficiency.\nTo reflect honestly and objectively on my internship process, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge and skills Understanding of the industry, applying knowledge to practice, skills in using tools, quality of work ✅ 2 Learning ability Acquiring new knowledge, learning quickly ✅ 3 Proactive Self-study, take on tasks without waiting for instructions ✅ 4 Sense of responsibility Complete work on time, ensure quality ✅ 5 Discipline Comply with working hours, rules, and procedures ✅ 6 Progressiveness Willingness to receive feedback and improve oneself ✅ 7 Communication Present ideas, report work clearly ✅ 8 Teamwork Work effectively with colleagues, participate in groups ✅ 9 Professional behavior Respect colleagues, partners, and the working environment ✅ 10 Problem-solving thinking Identify problems, propose solutions, and be creative ✅ 11 Contribution to the project/organization Work efficiency, innovation initiatives, recognition from the team ✅ 12 Overall General assessment of the entire internship process ✅ Needs Improvement Improve discipline: Need to be more aware of and strictly comply with the company\u0026rsquo;s internal regulations on time and procedures. Develop problem-solving thinking: Need to practice approaching problems in a more systematic and logical way, from analyzing causes to proposing feasible solutions. Improve communication skills: Need to be more confident and proactive in presenting opinions, discussing expertise as well as handling communication situations in daily work. "
},
{
	"uri": "http://localhost:1313/aws-workshop/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nFirst Cloud Journey\u0026rsquo;s environment is very professional and inspiring. The workshops are well organized with updated and practical content. The speakers and mentors are all experienced experts, enthusiastic in answering questions\n2. Support from Mentor / Team Admin\nMentor guides very detailedly, explains clearly when I don\u0026rsquo;t understand and always encourages me to ask questions. The admin team supports procedures, documents and creates favorable conditions for me to work smoothly. I appreciate that the mentor allows me to try and handle the problem myself instead of showing me how to do it.\n3. Relevance of Work to Academic Major\nThe work I was assigned was consistent with the knowledge I learned at school, and at the same time expanded to new areas that I had never been exposed to. Thanks to that, I not only consolidated my basic knowledge but also learned more practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and how to communicate professionally in a corporate environment. The mentor also shared many practical experiences to help me better orient my career.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still has fun. When there is an urgent project, everyone tries together, supports regardless of position. This helps me feel like I am part of the team, even though I am just an intern.\n6. Internship policy/benefits The company supports internship allowances and creates flexible working hours when needed. In addition, being able to participate in internal events is a big plus.\nMost satisfying I am most satisfied with the enthusiastic support from the mentor and the dynamic working environment of the program. The seniors are always ready to answer questions when I encounter difficulties, making me feel more comfortable and confident during the internship. I also appreciate the friendliness and cooperative spirit of everyone, helping me to integrate quickly and work more effectively, and understand more about Aws Cloud services.\n"
},
{
	"uri": "http://localhost:1313/aws-workshop/5-workshop/5.5-policy/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "title : \u0026ldquo;On-premise testing\u0026rdquo;\nweight : 5 chapter : false pre : \u0026quot; 5.5 \u0026quot; In this section, we will deploy and test the AI ​​model in an on-premise environment. This is an important step to evaluate the feasibility and performance of the system in real conditions before putting it into production 1.\nWe will connect via python (boto3) First we must know our region, Agent ID, Aliases Id\nMust have accesskey Id, secret key: Note that these 2 keys cannot be revealed, otherwise outsiders will use the service and you will lose money\nTry promt to see if it returns a result or not, if it does, it is considered successful. "
},
{
	"uri": "http://localhost:1313/aws-workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws-workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]